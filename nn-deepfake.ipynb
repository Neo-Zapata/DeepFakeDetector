{"cells":[{"cell_type":"code","execution_count":17,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2023-07-13T01:42:56.908479Z","iopub.status.busy":"2023-07-13T01:42:56.908041Z","iopub.status.idle":"2023-07-13T01:43:21.947125Z","shell.execute_reply":"2023-07-13T01:43:21.945978Z","shell.execute_reply.started":"2023-07-13T01:42:56.908438Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: efficientnet in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (1.1.1)\n","Requirement already satisfied: keras-applications<=1.0.8,>=1.0.7 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from efficientnet) (1.0.8)\n","Requirement already satisfied: scikit-image in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from efficientnet) (0.20.0)\n","Requirement already satisfied: numpy>=1.9.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet) (1.23.3)\n","Requirement already satisfied: h5py in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet) (3.8.0)\n","Requirement already satisfied: scipy>=1.8 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-image->efficientnet) (1.9.3)\n","Requirement already satisfied: networkx>=2.8 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-image->efficientnet) (3.1)\n","Requirement already satisfied: pillow>=9.0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-image->efficientnet) (9.2.0)\n","Requirement already satisfied: imageio>=2.4.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-image->efficientnet) (2.28.0)\n","Requirement already satisfied: tifffile>=2019.7.26 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-image->efficientnet) (2023.4.12)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-image->efficientnet) (1.4.1)\n","Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-image->efficientnet) (21.3)\n","Requirement already satisfied: lazy_loader>=0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-image->efficientnet) (0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from packaging>=20.0->scikit-image->efficientnet) (3.0.9)\n","Note: you may need to restart the kernel to use updated packages.\n","Requirement already satisfied: facenet-pytorch in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (2.5.3)\n","Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from facenet-pytorch) (1.23.3)\n","Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from facenet-pytorch) (2.28.2)\n","Requirement already satisfied: torchvision in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from facenet-pytorch) (0.15.2)\n","Requirement already satisfied: pillow in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from facenet-pytorch) (9.2.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->facenet-pytorch) (3.1.0)\n","Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->facenet-pytorch) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->facenet-pytorch) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->facenet-pytorch) (2023.5.7)\n","Requirement already satisfied: torch==2.0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torchvision->facenet-pytorch) (2.0.1)\n","Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch==2.0.1->torchvision->facenet-pytorch) (3.12.0)\n","Requirement already satisfied: typing-extensions in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch==2.0.1->torchvision->facenet-pytorch) (4.5.0)\n","Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch==2.0.1->torchvision->facenet-pytorch) (1.11.1)\n","Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch==2.0.1->torchvision->facenet-pytorch) (3.1)\n","Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch==2.0.1->torchvision->facenet-pytorch) (3.1.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jinja2->torch==2.0.1->torchvision->facenet-pytorch) (2.1.1)\n","Requirement already satisfied: mpmath>=0.19 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sympy->torch==2.0.1->torchvision->facenet-pytorch) (1.3.0)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install efficientnet\n","%pip install facenet-pytorch"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-07-13T01:43:21.950177Z","iopub.status.busy":"2023-07-13T01:43:21.949730Z","iopub.status.idle":"2023-07-13T01:43:21.961220Z","shell.execute_reply":"2023-07-13T01:43:21.959985Z","shell.execute_reply.started":"2023-07-13T01:43:21.950138Z"},"trusted":true},"outputs":[],"source":["import cv2\n","from facenet_pytorch import MTCNN\n","\n","# Code Extracted from: https://www.kaggle.com/code/timesler/fast-mtcnn-detector-55-fps-at-full-resolution\n","class FastMTCNN(object):\n","    \"\"\"Fast MTCNN implementation.\"\"\"\n","    \n","    def __init__(self, stride, resize=1, *args, **kwargs):\n","        \"\"\"Constructor for FastMTCNN class.\n","        \n","        Arguments:\n","            stride (int): The detection stride. Faces will be detected every `stride` frames\n","                and remembered for `stride-1` frames.\n","        \n","        Keyword arguments:\n","            resize (float): Fractional frame scaling. [default: {1}]\n","            *args: Arguments to pass to the MTCNN constructor. See help(MTCNN).\n","            **kwargs: Keyword arguments to pass to the MTCNN constructor. See help(MTCNN).\n","        \"\"\"\n","        self.stride = stride\n","        self.resize = resize\n","        self.mtcnn = MTCNN(*args, **kwargs)\n","        \n","    def __call__(self, frames):\n","        \"\"\"Detect faces in frames using strided MTCNN.\"\"\"\n","        if self.resize != 1:\n","            frames = [\n","                cv2.resize(f, (int(f.shape[1] * self.resize), int(f.shape[0] * self.resize)))\n","                    for f in frames\n","            ]\n","                      \n","        boxes, probs = self.mtcnn.detect(frames[::self.stride])\n","\n","        faces = []\n","        for i, frame in enumerate(frames):\n","            box_ind = int(i / self.stride)\n","            if boxes[box_ind] is None:\n","                continue\n","            for box in boxes[box_ind]:\n","                box = [int(b) for b in box]\n","                faces.append(frame[box[1]:box[3], box[0]:box[2]])\n","        \n","        return faces"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-07-13T01:43:21.963857Z","iopub.status.busy":"2023-07-13T01:43:21.963064Z","iopub.status.idle":"2023-07-13T01:43:34.019236Z","shell.execute_reply":"2023-07-13T01:43:34.018141Z","shell.execute_reply.started":"2023-07-13T01:43:21.963824Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["CUDA Available: False\n"]}],"source":["import cv2\n","from facenet_pytorch import MTCNN\n","import numpy as np\n","import h5py\n","import os\n","os.environ['TF_DISABLE_MMM'] = '1'  # Disable TensorFlow Meta Optimizer\n","from PIL import Image\n","import random\n","import tensorflow as tf\n","from efficientnet.keras import EfficientNetB0\n","from keras.models import Sequential\n","from keras.layers import LSTM, TimeDistributed, Dense, Conv2D, MaxPooling2D, Flatten, GlobalAveragePooling2D\n","import torch\n","\n","device = 'cpu'\n","random.seed(42)\n","\n","# 1 videos correspond to 10 frames (10 faces)\n","# each face is is represented as a matrix of 224x224 dimension. where each value is in rgb (3 values)\n","\n","FILE_NAME  = \"MODEL.h5\"\n","BATCH_SIZE = 10 # number of videos to process\n","cwd = os.getcwd()\n","face_detector = FastMTCNN(\n","stride=4,\n","resize=0.5,\n","margin=14,\n","factor=0.6,\n","keep_all=False,\n","device=device\n",")\n","\n","print('CUDA Available:', torch.cuda.is_available())\n","\n","def preprocess_image(image):\n","    image = image / 255.0 # normalize between [0, 1]\n","    return image\n","\n","def extract_faces(vid_path, batch, counter, n_faces = 10):\n","    video_path = os.path.join(cwd, vid_path)\n","    cap = cv2.VideoCapture(video_path)\n","    frame_count = 0\n","    face_images = []\n","    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","    frame_step = max(total_frames // n_faces, 1)\n","    \n","    frames = []\n","    while cap.isOpened():\n","        ret, frame = cap.read()\n","        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","        if not ret:\n","            break\n","        frame_count += 1\n","        if (frame_count - 1) % frame_step != 0:\n","            continue\n","        frames.append(frame)\n","        if len(frames) >= n_faces:\n","            break\n","    resized_faces = []\n","    if len(frames) > 0:\n","        faces = face_detector(frames)\n","        for face in faces:\n","            if len(resized_faces) >= n_faces:\n","                break\n","            if face.size == 0:\n","                continue\n","            resized_face = cv2.resize(face, (224, 224))\n","            resized_face = preprocess_image(resized_face)\n","            resized_faces.append(resized_face)\n","    cap.release()\n","    counter += 1\n","    print(counter)\n","    return resized_faces, counter\n","\n","\n","def get_splits(vids): # vids = (vid_name, label)\n","    train_ratio = 0.7\n","    val_ratio   = 0.15\n","    test_ratio  = 0.15\n","\n","    num_videos  = len(vids)\n","    num_train   = int(num_videos * train_ratio)\n","    num_val     = int(num_videos * val_ratio)\n","    num_test    = num_videos - num_train - num_val # ~0.15\n","\n","    train_videos = vids[:num_train]\n","    val_videos   = vids[num_train:num_train+num_val]\n","    test_videos  = vids[num_train+num_val:]\n","\n","    return train_videos, val_videos, test_videos\n","\n","# def save_faces(faces, dataset, batch): # faces = [(15 images), (15 images), ...]\n","#     with h5py.File(FILE_NAME, 'a') as f:\n","#         f.create_dataset(dataset + \"_\" + str(batch), data=faces)\n","#         f.close()\n","\n","def special_save(data, labels, label):\n","    with h5py.File(label + \"_data.h5\", \"w\") as f:\n","        for i, d in enumerate(data):\n","            f.create_dataset(f\"{label}_data_{i}\", data=d)\n","        f.create_dataset(label +\"_labels\", data=labels)\n","        f.close()\n","    print(f\"{label} data saved\")\n","\n","def load(label, dataset_keys):\n","    with h5py.File(cwd + \"/\" + label + \"_data.h5\", \"r\") as f:\n","        recovered_test_data = []\n","        for key in dataset_keys:\n","            if key != label + \"_labels\":\n","                data = f[key][:]\n","                recovered_test_data.append(data)\n","        f.close()\n","    return recovered_test_data\n","    \n","def special_load(label):\n","    with h5py.File(cwd + \"/\" + label + \"_data.h5\", \"r\") as f:\n","        recovered_test_labels = np.array(f[label + \"_labels\"])\n","        f.close()\n","    return recovered_test_labels\n","\n","def get_data_labels(vids, batches, dataset, counter, flag):\n","    data     = []\n","    labels   = []\n","    for batch_index in range(batches):\n","        batch_data = vids[batch_index * BATCH_SIZE : (batch_index + 1) * BATCH_SIZE]\n","        processed_faces = []\n","        # preprocess\n","        for video_paths, label in batch_data:\n","            faces, counter = extract_faces(video_paths, batch_index, counter) # list of faces\n","            data.append(np.array(faces))\n","            labels.append(1 if label == 'real' else 0)\n","            processed_faces.append(np.array(faces))\n","        # save\n","        if flag:\n","            pass\n","\n","    labels = np.array(labels)\n","    data = np.array(data)\n","    return data, labels, counter\n","\n","\n","def fix_shape(data):\n","    reshaped_data = []\n","    for arr in data:\n","        if arr.shape == (10, 224, 224, 3):\n","            reshaped_data.append(arr)\n","    reshaped_data = np.array(reshaped_data, dtype=np.float32)\n","    reshaped_data = reshaped_data.reshape((-1, 10, 224, 224, 3))\n","    return reshaped_data\n","\n","\n","def preprocessing():\n","    fake_path   = '/kaggle/input/dataset/archive/manipulated_sequences/Deepfakes/c23/videos'\n","    ff          = os.listdir(fake_path)\n","    fake_files  = [(fake_path + \"/\") + s for s in ff]\n","    fake_label  = ['fake'] * len(fake_files)\n","\n","    real_path   = '/kaggle/input/dataset/archive/original_sequences/youtube/c23/videos'\n","    rf          = os.listdir(real_path)\n","    real_files  = [(real_path + \"/\") + s for s in rf]\n","    real_label  = ['real'] * len(real_files)\n","\n","    random.seed(42)\n","\n","    fake_combined = list(zip(fake_files, fake_label))\n","    real_combined = list(zip(real_files, real_label))\n","    combined      = []\n","    combined.extend(fake_combined)\n","    combined.extend(real_combined)\n","    fake_combined = []\n","    real_combined = []\n","\n","    random.shuffle(combined)\n","\n","    train_vids, val_vids, test_vids = get_splits(combined)\n","    combined      = []\n","    \n","    # deleting half of the videos on each list for lack computational power\n","    del train_vids[len(train_vids) // 2:]\n","    del val_vids[len(val_vids) // 2:]\n","    del test_vids[len(test_vids) // 2:]\n","    \n","    print(f\"Training examples:   {len(train_vids)}\")\n","    print(f\"Validation examples: {len(val_vids)}\")\n","    print(f\"Testing examples:    {len(test_vids)}\")\n","    \n","    train_batches = len(train_vids) // BATCH_SIZE\n","    val_batches   = len(val_vids)   // BATCH_SIZE\n","    test_batches  = len(test_vids)  // BATCH_SIZE\n","    \n","    counter = 0\n","    \n","    train_data, train_labels, counter    = get_data_labels(train_vids, train_batches, 'train', counter, False)\n","#     train_data = fix_shape(train_data)\n","    special_save(train_data, train_labels, \"train\")\n","    print(\"train data saved\")\n","    train_data = []\n","    \n","    val_data, val_labels, counter        = get_data_labels(val_vids, val_batches, 'val', counter, False)\n","#     val_data   = fix_shape(val_data)\n","    special_save(val_data, val_labels, \"val\")\n","    print(\"val data saved\")\n","    val_data = []\n","    \n","    test_data, test_labels, counter      = get_data_labels(test_vids, test_batches, 'test', counter, False)\n","    special_save(test_data, test_labels, \"test\")\n","    print(\"test data saved\")\n","    test_data = []\n"]},{"cell_type":"code","execution_count":20,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-07-13T01:43:34.022250Z","iopub.status.busy":"2023-07-13T01:43:34.021562Z","iopub.status.idle":"2023-07-13T01:51:29.833705Z","shell.execute_reply":"2023-07-13T01:51:29.832652Z","shell.execute_reply.started":"2023-07-13T01:43:34.022210Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Testing\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," time_distributed (TimeDist  (None, 10, 7, 7, 1280)    4049564   \n"," ributed)                                                        \n","                                                                 \n"," time_distributed_1 (TimeDi  (None, 10, 1280)          0         \n"," stributed)                                                      \n","                                                                 \n"," lstm (LSTM)                 (None, 10, 64)            344320    \n","                                                                 \n"," flatten (Flatten)           (None, 640)               0         \n","                                                                 \n"," dense (Dense)               (None, 128)               82048     \n","                                                                 \n"," dense_1 (Dense)             (None, 1)                 129       \n","                                                                 \n","=================================================================\n","Total params: 4476061 (17.07 MB)\n","Trainable params: 4434045 (16.91 MB)\n","Non-trainable params: 42016 (164.12 KB)\n","_________________________________________________________________\n","5/5 [==============================] - 148s 25s/step - loss: 0.7652 - accuracy: 0.5270\n","Test Accuracy: 0.5270270109176636\n"]}],"source":["import os\n","import matplotlib.pyplot as plt\n","os.environ['TF_DISABLE_MMM'] = '1'  # Disable TensorFlow Meta Optimizer\n","import tensorflow as tf\n","from keras.models import load_model\n","from keras.utils import to_categorical\n","import gc\n","# from tensorflow.keras.utils import plot_model\n","\n","# gpus = tf.config.experimental.list_physical_devices('GPU')\n","# print(gpus)\n","\n","cwd = os.getcwd()\n","MODEL_FILE_NAME = cwd + \"/nn.h5\"\n","TRAIN_FILE_NAME  = \"/kaggle/input/data-processed/train_data.h5\"\n","VAL_FILE_NAME  = \"/kaggle/input/data-processed/val_data.h5\"\n","TEST_FILE_NAME  = cwd + \"/test_data.h5\"\n","\n","\n","def create_model():\n","    num_frames = 10\n","    input_shape     = (224, 224, 3)\n","    # the input has shape (10, 224, 224, 3)\n","\n","    #base model\n","    base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224,224,3)) \n","    # efficientenet b0 - cnn que acepta imagenes de 224x224x3\n","\n","    model = Sequential()\n","    # container\n","    model.add(TimeDistributed(base_model, input_shape=(num_frames,) + input_shape))\n","    # se aplica efficientnet a cada frame (convoluciones)\n","    model.add(TimeDistributed(GlobalAveragePooling2D()))\n","    # se realiza un average pooling a cada frame, reduciendo las dimensiones espaciales a un vector por frame.\n","    model.add(LSTM(units=64, return_sequences=True))\n","    # recibe los vectores caracteristicos de cada frame , los procesa y retorna vectores caracteristicos de 64 dim por cada frame\n","    model.add(Flatten())\n","    # transforma todos los feat vec a un solo feat vec\n","    model.add(Dense(units=128, activation='relu'))\n","    # pasa el feat vec a un MLP completo de 128 units\n","    model.add(Dense(units=1, activation='sigmoid'))\n","    # final layer que retorna la probibilidad de ser real o fake (1 = real video) (0 = fake video)\n","\n","    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","    model.summary()\n","\n","    return model\n","\n","def process_batches(data_list, labels_list, batch_size):\n","    for i in range(0, len(data_list), batch_size):\n","        batch_data = data_list[i:i+batch_size]\n","        batch_labels = labels_list[i:i+batch_size]\n","\n","        processed_data = [arr for arr in batch_data if arr.shape == (10, 224, 224, 3)]\n","        processed_labels = [label for arr, label in zip(batch_data, batch_labels) if arr.shape == (10, 224, 224, 3)]\n","\n","        yield processed_data, processed_labels\n","\n","if os.path.exists(MODEL_FILE_NAME):\n","    # testing\n","    print(\"Testing\")\n","\n","    model = tf.keras.models.load_model('nn.h5')\n","\n","    # Use the loaded model for prediction or further training\n","    \n","    with h5py.File(TEST_FILE_NAME, \"r\") as f:\n","        dataset_keys = list(f.keys())\n","        f.close()\n","        \n","    test_data = []\n","    test_labels = []\n","        \n","    test_data_list = load(\"test\", dataset_keys)\n","    test_labels_list = special_load(\"test\")\n","\n","    batch_size = 32\n","    batches = process_batches(test_data_list, test_labels_list, batch_size)\n","    del test_data_list\n","    del test_labels_list\n","    for batch_data, batch_labels in batches:\n","        batch_data = np.array(batch_data, dtype=np.float32)\n","        batch_data = fix_shape(batch_data)\n","        test_data.extend(batch_data)\n","        test_labels.extend(batch_labels)\n","    del batches\n","    gc.collect()\n","    \n","    test_data = np.array(test_data)\n","    test_labels = np.array(test_labels)\n","    # plot_model(model, to_file='model_architecture.png', show_shapes=True)\n","    model.summary()\n","    loss, accuracy = model.evaluate(test_data, test_labels)\n","    # print(\"Loss: \", loss)\n","    print(\"Test Accuracy:\", accuracy)\n","    \n","    \n","elif not os.path.exists(TRAIN_FILE_NAME):\n","    print(\"Preprocessing\")\n","    preprocessing()\n","else:\n","    # Load the model from the saved file\n","    # FINAL PREPROCESSING (limpieza)\n","    \n","    print(\"Clean data, create and train and validate model\")\n","    batch_size = 32\n","    \n","    half1 = []\n","    half2 = []\n","    half3 = []\n","    with h5py.File(\"/kaggle/input/data-processed/train_data.h5\", \"r\") as f:\n","        dataset_keys = list(f.keys())\n","        length = len(dataset_keys)\n","        half1 = dataset_keys[:length // 3]\n","        half2 = dataset_keys[length // 3: 2 * length // 3]\n","        half3 = dataset_keys[2 * length // 3:]\n","        f.close()\n","    \n","    del half3\n","    train_data = []\n","    train_labels = []\n","        \n","    train_data_list = load(\"train\", half1)\n","    train_labels_list = special_load(\"train\")\n","    train_labels_half1 = train_labels_list[:len(half1)]\n","    del train_labels_list\n","\n","    batches = process_batches(train_data_list, train_labels_half1, batch_size)\n","    del train_data_list\n","    del train_labels_half1\n","    for batch_data, batch_labels in batches:\n","        batch_data = np.array(batch_data, dtype=np.float32)\n","        batch_data = fix_shape(batch_data)\n","        train_data.extend(batch_data)\n","        train_labels.extend(batch_labels)\n","    del batches\n","    gc.collect()\n","    # ------\n","    \n","    train_data_list = load(\"train\", half2)\n","    train_labels_list = special_load(\"train\")\n","    train_labels_half2 = train_labels_list[len(half1):]\n","    del train_labels_list\n","\n","    batches = process_batches(train_data_list, train_labels_half2, batch_size)\n","    del train_data_list\n","    del train_labels_half2\n","    for batch_data, batch_labels in batches:\n","        batch_data = np.array(batch_data, dtype=np.float32)\n","        batch_data = fix_shape(batch_data)\n","        train_data.extend(batch_data)\n","        train_labels.extend(batch_labels)\n","    del batches\n","\n","    train_data = np.array(train_data)\n","    train_labels = np.array(train_labels)\n","#     train_labels = to_categorical(train_labels)\n","\n","    print(train_data.shape)\n","    print(train_labels.shape)\n","    print(\"done with train data\")    \n","    gc.collect()\n","\n","# ---------------------------------------------------------------------\n","#     half1 = []\n","#     half2 = []\n","#     with h5py.File(\"/kaggle/input/data-processed/val_data.h5\", \"r\") as f:\n","#         dataset_keys = list(f.keys())\n","#         half1 = dataset_keys[:len(dataset_keys)//2]\n","#         half2 = dataset_keys[len(dataset_keys)//2:]\n","#         f.close()\n","        \n","#     val_data = []\n","#     val_labels = []\n","        \n","#     train_data_list = load(\"val\", half1)\n","#     train_labels_list = special_load(\"val\")\n","#     train_labels_half1 = train_labels_list[:len(half1)]\n","#     del train_labels_list\n","\n","#     batches = process_batches(train_data_list, train_labels_half1, batch_size)\n","#     del train_data_list\n","#     del train_labels_half1\n","#     for batch_data, batch_labels in batches:\n","#         batch_data = np.array(batch_data, dtype=np.float32)\n","#         batch_data = fix_shape(batch_data)\n","#         val_data.extend(batch_data)\n","#         val_labels.extend(batch_labels)\n","#     del batches\n","#     gc.collect()\n","#     # ------\n","    \n","#     train_data_list = load(\"val\", half2)\n","#     train_labels_list = special_load(\"val\")\n","#     train_labels_half2 = train_labels_list[len(half1):]\n","#     del train_labels_list\n","\n","#     batches = process_batches(train_data_list, train_labels_half2, batch_size)\n","#     del train_data_list\n","#     del train_labels_half2\n","#     for batch_data, batch_labels in batches:\n","#         batch_data = np.array(batch_data, dtype=np.float32)\n","#         batch_data = fix_shape(batch_data)\n","#         val_data.extend(batch_data)\n","#         val_labels.extend(batch_labels)\n","#     del batches\n","#     gc.collect()\n","#     val_data = np.array(val_data, dtype=np.float32)\n","#     val_labels = np.array(val_labels)\n","#     val_labels = to_categorical(val_labels)\n","\n","#     print(val_data.shape)\n","#     print(val_labels.shape)\n","#     print(\"done with val data\")    \n","    \n","    # CREATE MODEL\n","    model = create_model()\n","    print(\"model created\")\n","#     model.fit(train_data, train_labels, batch_size=2, epochs=10, validation_data=(val_data, val_labels))\n","    history = model.fit(train_data, train_labels, batch_size=8, epochs=10)\n","    print(\"model trained\")\n","    \n","    # Plot training loss\n","    plt.plot(history.history['loss'])\n","    plt.title('Training Loss')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Loss')\n","    plt.savefig('training_loss.png')  # Save the plot as an image file\n","    plt.show()\n","\n","    # Plot training accuracy\n","    plt.plot(history.history['accuracy'])\n","    plt.title('Training Accuracy')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Accuracy')\n","    plt.savefig('training_accuracy.png')  # Save the plot as an image file\n","    plt.show()\n","    \n","    del train_data\n","    del train_labels\n","#     del val_data\n","#     del val_labels\n","    \n","    model.save(\"nn.h5\")\n","    print(\"model saved\")\n","\n","    with h5py.File(\"/kaggle/input/data-processed/test_data.h5\", \"r\") as f:\n","        dataset_keys = list(f.keys())\n","        f.close()\n","        \n","    test_data = []\n","    test_labels = []\n","        \n","    test_data_list = load(\"test\", dataset_keys)\n","    test_labels_list = special_load(\"test\")\n","\n","    batch_size = 32\n","    batches = process_batches(test_data_list, test_labels_list, batch_size)\n","    del test_data_list\n","    del test_labels_list\n","    for batch_data, batch_labels in batches:\n","        batch_data = np.array(batch_data, dtype=np.float32)\n","        batch_data = fix_shape(batch_data)\n","        test_data.extend(batch_data)\n","        test_labels.extend(batch_labels)\n","    del batches\n","    gc.collect()\n","    \n","    test_data = np.array(test_data)\n","    test_labels = np.array(test_labels)\n","        \n","    loss, accuracy = model.evaluate(test_data, test_labels)\n","    # print(\"Loss: \", loss)\n","    print(\"Test Accuracy:\", accuracy)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.7"}},"nbformat":4,"nbformat_minor":4}
